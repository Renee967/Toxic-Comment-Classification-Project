{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# Toxic Comment Classification\n","\n","Data Augmentation\n","\n","#### Dissertation Project\n","### Name: Renee Mendonca\n","### Student Number-221040908\n"],"metadata":{"id":"F0TQL2WbRoAK"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h5iconTrftIg","executionInfo":{"status":"ok","timestamp":1722424470165,"user_tz":-60,"elapsed":26615,"user":{"displayName":"Rhea Mendonca","userId":"14546682104430249197"}},"outputId":"ab750be0-0bc8-4b30-f68c-0cbb2fcf5e15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import nltk\n","from nltk.corpus import wordnet\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Ensure necessary NLTK data files are downloaded\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","# Update the file paths to point to your Google Drive\n","training_data_file = \"/content/drive/My Drive/toxic-comment-classification/OLIDv1/olid-training-v1.0.tsv\"\n","labels_b_file = \"/content/drive/My Drive/toxic-comment-classification/OLIDv1/labels-levelb.csv\"\n","\n","# Load training data\n","try:\n","    training_data = pd.read_csv(training_data_file, delimiter='\\t')\n","    labels_b = pd.read_csv(labels_b_file, header=None, names=['id', 'label'])\n","except FileNotFoundError as e:\n","    print(f\"Error loading files: {e}\")\n","    exit(1)\n","\n","# Print the first few rows to verify\n","print(\"Training data head:\")\n","print(training_data.head())\n","print(\"Labels B head:\")\n","print(labels_b.head())\n","\n","# Print column names to debug the issue\n","print(\"Training data columns:\")\n","print(training_data.columns)\n","print(\"Labels B columns:\")\n","print(labels_b.columns)\n","\n","# Merge data to get labels for Task B\n","training_data = training_data.rename(columns={\"id\": \"id\", \"tweet\": \"comment\"})\n","# Check for common IDs\n","print(\"Unique IDs in training_data:\", training_data['id'].unique())\n","print(\"Unique IDs in labels_b:\", labels_b['id'].unique())\n","\n","merged_data = training_data.merge(labels_b, on='id', how='inner')  # or 'left', 'right', 'outer'\n","tin_unt_data = merged_data[merged_data['label'].isin(['TIN', 'UNT'])]\n","\n","# Verify the merged data\n","print(\"Merged Data Sample:\")\n","print(tin_unt_data.head())\n","print(tin_unt_data.columns)\n","\n","def get_synonym(word, pos):\n","    \"\"\"Gets a synonym for a given word and its part of speech.\"\"\"\n","    replacements = []\n","    for syn in wordnet.synsets(word):\n","        if pos[1] == 'NNP' or pos[1] == 'DT':\n","            break\n","        word_type = pos[1][0].lower()\n","        if syn.name().find(\".\" + word_type + \".\"):\n","            r = syn.name()[0:syn.name().find(\".\")]\n","            replacements.append(r)\n","    if len(replacements) > 0:\n","        replacement = replacements[np.random.randint(0, len(replacements))]\n","        return replacement\n","    else:\n","        return word\n","\n","def replace_words(words):\n","    \"\"\"Replaces random words in a list with their synonyms.\"\"\"\n","    tagged = nltk.pos_tag(words)\n","    output = \"\"\n","    for i in range(0, len(words)):\n","        replacements = []\n","        for syn in wordnet.synsets(words[i]):\n","            if tagged[i][1] == 'NNP' or tagged[i][1] == 'DT':\n","                break # Indentation fixed here\n","            word_type = tagged[i][1][0].lower()\n","            if syn.name().find(\".\" + word_type + \".\"):\n","                r = syn.name()[0:syn.name().find(\".\")]\n","                replacements.append(r)\n","        if len(replacements) > 0:\n","            replacement = replacements[np.random.randint(0, len(replacements))]\n","            output += \" \" + replacement\n","        else:\n","            output += \" \" + words[i]\n","    return [output]\n","\n","def replace_random_words(list):\n","    \"\"\"Replaces random words in a list with their synonyms.\"\"\"\n","    tagged = nltk.pos_tag(list)\n","    num_words_to_replace = int(np.floor(0.5 * len(list)))\n","    random_indices = generate_random_word_indices(num_words_to_replace, len(list))\n","    new_comments = []\n","    for i in range(1, num_words_to_replace + 1):\n","        words_to_replace = random_indices[0:i]\n","        new_comment = []\n","        for j in range(0, len(list)):\n","            if j not in words_to_replace:\n","                new_comment.append(list[j])\n","            else:\n","                new_comment.append(get_synonym(list[j], tagged[j]))\n","        new_comments.append(' '.join(new_comment))\n","    return new_comments\n","\n","def unique_comment(list):\n","    \"\"\"Removes duplicate words from a list.\"\"\"\n","    return list(set(list))\n","\n","def generate_random_word_indices(num_words_to_remove, comment_length):\n","    \"\"\"Generates random indices for word removal or replacement.\"\"\"\n","    random_indices = []\n","    for _ in range(num_words_to_remove):\n","        random_index = np.random.randint(low=0, high=comment_length)\n","        while random_index in random_indices:\n","            random_index = np.random.randint(low=0, high=comment_length)\n","        random_indices.append(random_index)\n","    return random_indices\n","\n","def remove_random_words(list):\n","    \"\"\"Removes random words from a list.\"\"\"\n","    num_words_to_remove = int(np.floor(0.2 * len(list)))\n","    random_indices = generate_random_word_indices(num_words_to_remove, len(list))\n","    new_comments = []\n","    for i in range(1, num_words_to_remove + 1):\n","        words_to_remove = random_indices[0:i]\n","        new_comment = []\n","        for j in range(0, len(list)):\n","            if j not in words_to_remove:\n","                new_comment.append(list[j])\n","        new_comments.append(' '.join(new_comment))\n","    return new_comments\n","\n","# Augment data\n","new_training_comments = []\n","new_training_labels = []\n","\n","for i in range(len(tin_unt_data)):\n","    comment = tin_unt_data.iloc[i]['comment']\n","    label = tin_unt_data.iloc[i]['label']\n","    comment_list = [word for word in comment.split()]\n","    comment_list = list(filter(None, comment_list))\n","    unique_comment_list = unique_comment(comment_list)\n","    new_comments_after_removal = remove_random_words(comment_list)\n","    new_comments_after_replacement = replace_random_words(comment_list)\n","\n","    new_comments_after_removal.append(comment)\n","    new_comments_after_removal.append(' '.join(unique_comment_list))\n","\n","    # Correctly append to lists\n","    new_training_comments.extend(new_comments_after_removal)\n","    new_training_labels.extend([label] * len(new_comments_after_removal))\n","\n","    new_training_comments.extend(new_comments_after_replacement)\n","    new_training_labels.extend([label] * len(new_comments_after_replacement))\n","\n","new_training_comments = np.array([new_training_comments]).transpose()\n","new_training_labels = np.array(new_training_labels)\n","\n","training_data_augmented = np.append(new_training_comments.reshape((-1, 1)), new_training_labels.reshape((-1, 1)), axis=1)\n","\n","# Print some of the augmented data before saving\n","print(\"Sample augmented data:\")\n","print(training_data_augmented[:5])\n","\n","# Save augmented data\n","output_file = \"/content/drive/My Drive/toxic-comment-classification/OLIDv1/training-data-augmented-all-task-b.csv\"\n","np.savetxt(output_file, training_data_augmented, delimiter=',', fmt='%s')\n","print(f\"Augmented data saved to {output_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KBZvd0yPjEjA","executionInfo":{"status":"ok","timestamp":1722427600910,"user_tz":-60,"elapsed":2186,"user":{"displayName":"Rhea Mendonca","userId":"14546682104430249197"}},"outputId":"515d1327-9737-41a9-df3b-eaef3c3dfc1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Training data head:\n","      id                                              tweet subtask_a  \\\n","0  86426  @USER She should ask a few native Americans wh...       OFF   \n","1  90194  @USER @USER Go home youâ€™re drunk!!! @USER #MAG...       OFF   \n","2  16820  Amazon is investigating Chinese employees who ...       NOT   \n","3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n","4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n","\n","  subtask_b subtask_c  \n","0       UNT       NaN  \n","1       TIN       IND  \n","2       NaN       NaN  \n","3       UNT       NaN  \n","4       NaN       NaN  \n","Labels B head:\n","      id label\n","0  15923   TIN\n","1  60133   TIN\n","2  83681   TIN\n","3  65507   TIN\n","4  12588   UNT\n","Training data columns:\n","Index(['id', 'tweet', 'subtask_a', 'subtask_b', 'subtask_c'], dtype='object')\n","Labels B columns:\n","Index(['id', 'label'], dtype='object')\n","Unique IDs in training_data: [86426 90194 16820 ... 82921 27429 46552]\n","Unique IDs in labels_b: [15923 60133 83681 65507 12588 34263 49139 58995 88490 46444 60587 70569\n"," 44546 51628 30899 40110 15998 96457 70841 46139 80947 40386 98916 32190\n"," 27550 24040 73516 88905 42112 37740 72405 47445 27158 17183 42325 15180\n"," 58026 81352 33133 91036 22882 40842 73612 15815 24049 79204 46229 42196\n"," 34669 34575 20357 52445 10991 57135 72369 42192 43587 10684 41553 62986\n"," 70051 73642 10727 11645 72274 21524 84579 59519 51386 10918 57541 25125\n"," 61295 53351 25463 85613 85886 54621 80397 68875 67926 41588 51948 28196\n"," 50376 45304 27228 57326 88221 17714 97610 63048 30075 60466 25685 16333\n"," 78417 51525 84550 23530 57284 59700 13433 80197 79309 47696 71294 55633\n"," 87934 57732 27455 83466 48657 89635 89603 86611 12193 63985 20522 94260\n"," 70324 89329 11100 83240 81890 72037 78301 94607 72531 98575 58342 39429\n"," 18786 35785 67049 82273 98216 37649 14582 59751 92215 22311 75125 21354\n"," 22594 19410 33394 56513 59691 41997 56637 19815 97410 77746 17704 61674\n"," 39860 21826 79934 55048 76833 55832 32056 67841 94427 96594 62689 35968\n"," 14516 15866 57185 91430 29203 27913 53325 46410 26072 16856 89200 45269\n"," 15437 91472 98531 43753 98685 31665 38732 46983 22067 79756 99563 23542\n"," 83416 67418 58632 35612 96905 79778 90328 71350 43782 58287 32061 63129\n"," 74797 25177 87428 39400 62788 88745 70443 90327 65545 58543 79222 72401\n"," 31354 29008 41590 72523 14640 74909 96397 34030 69073 29113 11286 41821\n"," 76379 52080 51762 71592 78688 76135 30778 22569 48938 41438 73439 67018]\n","Merged Data Sample:\n","Empty DataFrame\n","Columns: [id, comment, subtask_a, subtask_b, subtask_c, label]\n","Index: []\n","Index(['id', 'comment', 'subtask_a', 'subtask_b', 'subtask_c', 'label'], dtype='object')\n","Sample augmented data:\n","[]\n","Augmented data saved to /content/drive/My Drive/toxic-comment-classification/OLIDv1/training-data-augmented-all-task-b.csv\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["Load the Augmented Data\n"],"metadata":{"id":"Bk41owS8kJxV"}},{"cell_type":"code","source":["# Check the content of the CSV file\n","!head /content/drive/My\\ Drive/toxic-comment-classification/OLIDv1/training-data-augmented-all-task-b.csv\n"],"metadata":{"id":"aYPulGqjlVih"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uiCRZLbEIkOA","colab":{"base_uri":"https://localhost:8080/","height":554},"executionInfo":{"status":"error","timestamp":1722430287948,"user_tz":-60,"elapsed":263,"user":{"displayName":"Rhea Mendonca","userId":"14546682104430249197"}},"outputId":"b202ad05-30da-4c3c-ef8b-17780af5097e"},"source":["from nltk.corpus import wordnet\n","import pandas as pd\n","import numpy as np\n","import string\n","import nltk\n","\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","# Load data using pandas\n","data = pd.read_csv('/content/drive/My Drive/toxic-comment-classification/OLIDv1/training-data-augmented-all-task-b.csv', header=None)\n","\n","# Convert to numpy array if needed\n","training_data = data.values\n","\n","# Check the shape of the array\n","print(training_data.shape)\n","\n","# Access data based on the shape\n","if training_data.ndim == 2:\n","    list_sentences_training = training_data[:, 1]  # Assuming the second column is the text\n","else:\n","    # Handle 1D array case or reshape if necessary\n","    pass\n","# Get word Synonym\n","def get_synonym(word, pos):\n","\n","    replacements = []\n","    # Only replace nouns with nouns, vowels with vowels etc.\n","    for syn in wordnet.synsets(word):\n","\n","        # Do not attempt to replace proper nouns or determiners\n","        if pos[1] == 'NNP' or pos[1] == 'DT':\n","            break\n","\n","        # The tokenizer returns strings like NNP, VBP etc\n","        # but the wordnet synonyms has tags like .n.\n","        # So we extract the first character from NNP ie n\n","        # then we check if the dictionary word has a .n. or not\n","        word_type = pos[1][0].lower()\n","        if syn.name().find(\".\" + word_type + \".\"):\n","            # extract the word only\n","            r = syn.name()[0:syn.name().find(\".\")]\n","            replacements.append(r)\n","\n","    if len(replacements) > 0:\n","        # Choose a random replacement\n","#         print(word, 'replacemeents', replacements)\n","        replacement = replacements[np.random.randint(0, len(replacements))]\n","        return replacement\n","    else:\n","        # If no replacement could be found, then just use the\n","        # original word\n","        return word\n","\n","# Replace Random Words with their synonyms\n","def replace_words(words):\n","  # Identify the parts of speech\n","  tagged = nltk.pos_tag(words)\n","\n","  output = \"\"\n","\n","  for i in range(0, len(words)):\n","      replacements = []\n","\n","      # Only replace nouns with nouns, vowels with vowels etc.\n","      for syn in wordnet.synsets(words[i]):\n","\n","          # Do not attempt to replace proper nouns or determiners\n","          if tagged[i][1] == 'NNP' or tagged[i][1] == 'DT':\n","              break\n","\n","          # The tokenizer returns strings like NNP, VBP etc\n","          # but the wordnet synonyms has tags like .n.\n","          # So we extract the first character from NNP ie n\n","          # then we check if the dictionary word has a .n. or not\n","          word_type = tagged[i][1][0].lower()\n","          if syn.name().find(\".\" + word_type + \".\"):\n","              # extract the word only\n","              r = syn.name()[0:syn.name().find(\".\")]\n","              replacements.append(r)\n","\n","      if len(replacements) > 0:\n","          # Choose a random replacement\n","          replacement = replacements[np.random.randint(0, len(replacements))]\n","          output = output + \" \" + replacement\n","      else:\n","          # If no replacement could be found, then just use the\n","          # original word\n","          output = output + \" \" + words[i]\n","\n","      return [output]\n","\n","def replace_random_words(list):\n","  # Identify the parts of speech\n","  tagged = nltk.pos_tag(list)\n","\n","  num_words_to_replace = int(np.floor(0.5 * len(list)))\n","  # print(len(list))\n","  # print(num_words_to_remove)\n","  random_indices = generate_random_word_indices(num_words_to_replace, len(list))\n","  # print(random_indices)\n","#     new_comments = [' '.join(list)]\n","  new_comments = []\n","  for i in range(1, num_words_to_replace + 1):\n","      words_to_replace = random_indices[0:i]\n","      new_comment = []\n","      for j in range(0, len(list)):\n","          if j not in words_to_replace:\n","              new_comment.append(list[j])\n","          else:\n","              new_comment.append(get_synonym(list[j], tagged[j]))\n","      # print(new_comment)\n","      new_comments.append(' '.join(new_comment))\n","\n","  return new_comments\n","\n","\n","# Remove repeated words\n","def unique_comment(list):\n","    ulist = []\n","    [ulist.append(x) for x in list if x not in ulist]\n","    return ulist\n","\n","\n","# generate words indices to remove or replace\n","def generate_random_word_indices(num_words_to_remove, comment_length):\n","    random_indices = []\n","    # print('words to remove = ', num_words_to_remove)\n","    for i in range(0, num_words_to_remove):\n","        random_index = np.random.randint(low=0, high=comment_length)\n","        while random_index in random_indices:\n","#             print('random index', random_index)\n","#             print('random indices', random_indices)\n","            random_index = np.random.randint(low=0, high=comment_length)\n","\n","        random_indices.append(random_index)\n","\n","    return random_indices\n","\n","\n","# Remove Random Words\n","def remove_random_words(list):\n","    num_words_to_remove = int(np.floor(0.2 * len(list)))\n","    # print(len(list))\n","    # print(num_words_to_remove)\n","    random_indices = generate_random_word_indices(num_words_to_remove, len(list))\n","    # print(random_indices)\n","#     new_comments = [' '.join(list)]\n","    new_comments = []\n","    for i in range(1, num_words_to_remove + 1):\n","        words_to_remove = random_indices[0:i]\n","        new_comment = []\n","        for j in range(0, len(list)):\n","            if j not in words_to_remove:\n","                new_comment.append(list[j])\n","        # print(new_comment)\n","        new_comments.append(' '.join(new_comment))\n","\n","    return new_comments\n","\n","\n","# Severe Toxic, Threat and Identity Hate are the ones with small number of records\n","training_data = np.loadtxt('/content/drive/My Drive/toxic-comment-classification/OLIDv1/training-data-augmented-all-task-b.csv', delimiter=',', dtype=str)\n","\n","# Get the comment part of the data\n","list_sentences_training = training_data[:,1]\n","\n","# Get the labels part of the data\n","training_labels = training_data[:,3]\n","\n","new_training_comments = []\n","new_training_labels = []\n","print(list_sentences_training.shape[0])\n","\n","new_comments = 0\n","for i in range(0, list_sentences_training.shape[0]):\n","    if training_labels[i] == 'UNT':\n","        # comment_list = [word.strip(string.punctuation) for word in list_sentences_training[i].split()]\n","        comment_list = [word for word in list_sentences_training[i].split()]\n","\n","        # remove empty strings\n","        comment_list = list(filter(None, comment_list))\n","        unique_comment_list = unique_comment(comment_list)\n","        new_comments_after_removal = remove_random_words(comment_list)\n","        new_comments_after_replacement = replace_random_words(comment_list)\n","\n","        new_comments_after_removal.append(list_sentences_training[i])\n","        new_comments_after_removal.append(' '.join(unique_comment_list))\n","\n","        [new_training_comments.append(new_comment) for new_comment in new_comments_after_removal]\n","        [new_training_labels.append(training_labels[i]) for new_comment in new_comments_after_removal]\n","\n","        [new_training_comments.append(new_comment) for new_comment in new_comments_after_replacement]\n","        [new_training_labels.append(training_labels[i]) for new_comment in new_comments_after_replacement]\n","        new_comments += len(new_comments_after_removal) + len(new_comments_after_replacement) - 1\n","\n","    elif training_labels[i] == 'TIN':\n","        new_training_comments.append(list_sentences_training[i])\n","        new_training_labels.append(training_labels[i])\n","\n","new_training_comments = np.array([new_training_comments]).transpose()\n","print(new_training_comments.shape)\n","\n","new_training_labels = np.array(new_training_labels)\n","print(new_training_labels.shape)\n","print(new_training_labels[5])\n","\n","training_data_augemented = np.append(new_training_comments.reshape((-1, 1)), new_training_labels.reshape((-1, 1)), axis=1)\n","print('shape of augemented training data', training_data_augemented.shape)\n","print(training_data_augemented[0])\n","print(training_data_augemented[1])\n","\n","print('number of newly added comments', new_comments)\n","np.savetxt('/content/drive/My Drive/toxic-comment-classification/OLIDv1/training-data-augmented-all-task-b.csv', training_data_augemented, delimiter=',', fmt='%s')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"error","ename":"EmptyDataError","evalue":"No columns to parse from file","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-820e3b6affd4>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load data using pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/toxic-comment-classification/OLIDv1/training-data-augmented-all-task-b.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Convert to numpy array if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1723\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1724\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Fail here loudly instead of in cython after reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"]}]},{"cell_type":"code","source":[],"metadata":{"id":"UgHXNSmB2GyQ"},"execution_count":null,"outputs":[]}]}